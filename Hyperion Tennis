import os
import shutil
import zipfile
import pandas as pd
import glob
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
import xgboost as xgb
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import time
import re
import json
import sys
import csv
from unicodedata import normalize

print(f"XGBoost version: {xgb.__version__}")

player_stats_cache = {}

kaggle_json_path = r"C:\Users\Matth\Downloads\kaggle.json"
dataset_zip_path = r"C:\Users\Matth\Downloads\atp-matches-dataset.zip"
stats_json_path = r"C:\Users\Matth\MyProject\atp_player_stats_2025.json"

os.makedirs(os.path.dirname(stats_json_path), exist_ok=True)

try:
    service = Service('C:/Users/Matth/Downloads/chromedriver.exe')
    options = Options()
    driver = webdriver.Chrome(service=service, options=options)
except Exception as e:
    print(f"Error setting up Chrome WebDriver: {e}")
    print("Ensure chromedriver.exe is in C:\\Users\\Matth\\Downloads or install webdriver-manager")
    exit(1)

def normalize_name(name):
    """Normalize player name (remove accents, convert to lowercase, replace spaces with hyphens)"""
    name = normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')
    return name.lower().replace(' ', '-')

def scrape_atp_player(player_name):
    url = "https://www.atptour.com/en/rankings/singles?rankRange=0-5000"
    try:
        driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "mega-table")))
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        rows = soup.select("table.mega-table tbody tr")
        normalized_input = normalize_name(player_name)
        for row in rows:
            cell = row.select_one("td.player a")
            points_cell = row.select_one("td.points")
            if not cell or not points_cell:
                continue
            href = cell['href']
            if "/players/" not in href:
                continue
            segments = href.strip("/").split("/")
            if len(segments) >= 4:
                name_segment = segments[2]
                code_segment = segments[3]
                name = '-'.join([part.capitalize() for part in name_segment.split('-')])
                if normalize_name(name) == normalized_input:
                    points = points_cell.text.strip().replace(',', '')
                    try:
                        points = float(points) if points.replace('.', '').isdigit() else 0.0
                        return {"name": name, "code": code_segment, "rank_points": points}
                    except ValueError:
                        print(f"Invalid rank points for {player_name}: {points}")
                        return None
        print(f"Player {player_name} not found in ATP rankings (0-5000). Skipping.")
        return None
    except Exception as e:
        print(f"Error scraping ranking for {player_name}: {e}")
        return None

def scrape_tennisstats_player(player_name):
    url = f"https://tennisstats.com/players/{normalize_name(player_name)}"
    try:
        driver.get(url)
        WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, "div")))
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        stats = {
            "aces_per_match": None,
            "breaks_per_match_bo3": None
        }

        divs = soup.find_all('div', class_=re.compile(r'w25.*fl.*item.*ac'))
        break_assigned = False
        for div in divs:
            text = div.text.strip().lower()
            match = re.search(r'\d+\.\d+', text)
            if match and "%" not in text:
                number = float(match.group())
                if "aces" in text and stats["aces_per_match"] is None:
                    stats["aces_per_match"] = round(number)
                elif re.search(r'break|breaks', text) and not break_assigned:
                    stats["breaks_per_match_bo3"] = round(number)
                    break_assigned = True
                if stats["aces_per_match"] is not None and break_assigned:
                    break

        return stats if any(stats.values()) else None
    except Exception as e:
        print(f"Error scraping tennisstats.com for {player_name}: {e}")
        return None

def scrape_atptour_player_stats(player_name, player_code):
    url = f"https://www.atptour.com/en/players/{normalize_name(player_name)}/{player_code}/player-stats?year=2025&surface=all"
    max_attempts = 2
    attempt = 1
    
    while attempt <= max_attempts:
        try:
            driver.refresh()
            driver.get(url)
            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, "stats_items")))
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            stats = {
                "first_serve_points_won": None,
                "second_serve_points_won": None,
                "service_games_played": None
            }

            items = soup.find_all('li', class_='stats_items')
            for item in items:
                label = item.find('span', class_='stats_record')
                value = item.find('span', class_='stats_percentage')
                if label and value:
                    text = label.text.strip().lower()
                    value_text = value.text.strip()
                    if text == '1st serve points won':
                        stats["first_serve_points_won"] = float(value_text.strip('%')) / 100 if '%' in value_text else None
                    elif text == '2nd serve points won':
                        stats["second_serve_points_won"] = float(value_text.strip('%')) / 100 if '%' in value_text else None
                    elif text == 'service games played':
                        stats["service_games_played"] = float(value_text) if value_text.replace('.', '').isdigit() else None

            print(f"Attempt {attempt} for {player_name}: 1st Serve Pts Won (raw %): {stats['first_serve_points_won']}, 2nd Serve Pts Won (raw %): {stats['second_serve_points_won']}, Service Games: {stats['service_games_played']}")

            if stats["service_games_played"] is None:
                print(f"Attempt {attempt} failed for {player_name}: service_games_played is None. Retrying...")
                attempt += 1
                time.sleep(2)
            else:
                break
        except Exception as e:
            print(f"Attempt {attempt} failed for {player_name}: {e}")
            attempt += 1
            time.sleep(2)

    if stats["service_games_played"] is None:
        print(f"Failed to scrape service_games_played for {player_name} after {max_attempts} attempts.")
        return None

    if stats["first_serve_points_won"] is not None and stats["service_games_played"] is not None:
        stats["first_serve_points_won"] = round((stats["first_serve_points_won"] * stats["service_games_played"]) / 6)
    if stats["second_serve_points_won"] is not None and stats["service_games_played"] is not None:
        stats["second_serve_points_won"] = round((stats["second_serve_points_won"] * stats["service_games_played"]) / 6)

    del stats["service_games_played"]

    return stats if any(stats.values()) else None

def scrape_and_save_stats(players):
    all_data = []
    valid_players = []
    for player_name in players:
        try:
            player = scrape_atp_player(player_name)
            if not player:
                print(f"Skipping {player_name} due to missing or invalid rank points.")
                continue
            atptour_stats = scrape_atptour_player_stats(player_name, player['code'])
            tennisstats_stats = scrape_tennisstats_player(player_name)
            if not atptour_stats or not tennisstats_stats:
                print(f"Skipping {player_name} due to missing stats.")
                continue
            player.update(atptour_stats)
            player.update(tennisstats_stats)
            print(f"Scraped {player['name']}: Rank Points={player['rank_points']}, 1st Serve Pts Won={player['first_serve_points_won']}, 2nd Serve Pts Won={player['second_serve_points_won']}, Aces={player['aces_per_match']}, Breaks(Bo3)={player['breaks_per_match_bo3']}")
            all_data.append(player)
            player_stats_cache[player['name'].lower()] = player
            valid_players.append(player_name)
            time.sleep(2)
        except Exception as e:
            print(f"Error scraping {player_name}: {e}")
            print(f"Skipping {player_name}.")
            continue

    if not all_data:
        print("No valid player stats scraped. Exiting.")
        return [], []

    try:
        with open(stats_json_path, "w", encoding="utf-8") as f:
            json.dump(all_data, f, indent=2)
        print(f"Saved player stats to {stats_json_path}")
    except Exception as e:
        print(f"Error saving JSON to {stats_json_path}: {e}")
        print("Continuing with in-memory stats.")
    return all_data, valid_players

def check_feature_alignment(df, players):
    live_features = []
    for player_name in players:
        stats = load_player_stats(player_name)
        if stats:
            live_features.append([
                float(stats['first_serve_points_won']),
                float(stats['second_serve_points_won']),
                float(stats['aces_per_match']),
                float(stats['breaks_per_match_bo3']),
                np.log1p(float(stats['rank_points']))
            ])
    live_features = np.array(live_features) if live_features else np.array([])
    
    if live_features.size == 0:
        print("No valid live features for comparison.")
        return {}
    
    dataset_features = df[['w_1stWon', 'w_2ndWon', 'w_ace', 'w_bpSaved', 'winner_rank_points']].values
    dataset_features[:, -1] = np.log1p(dataset_features[:, -1])
    dataset_means = dataset_features.mean(axis=0)
    dataset_stds = dataset_features.std(axis=0)
    live_means = live_features.mean(axis=0)
    live_stds = live_features.std(axis=0)
    
    print("Feature Alignment Check:")
    print(f"{'Feature':<20} {'Dataset Mean':<15} {'Live Mean':<15} {'Dataset Std':<15} {'Live Std':<15} {'Suggested Scale':<15}")
    feature_names = ['1st_serve_won', '2nd_serve_won', 'aces', 'breaks_per_match', 'rank_points']
    scale_factors = {}
    for i, name in enumerate(feature_names):
        scale = dataset_means[i] / live_means[i] if live_means[i] != 0 else 1.0
        scale_factors[name] = scale
        print(f"{name:<20} {dataset_means[i]:<15.2f} {live_means[i]:<15.2f} {dataset_stds[i]:<15.2f} {live_stds[i]:<15.2f} {scale:<15.2f}")
    return scale_factors

try:
    os.makedirs(os.path.expanduser("~/.kaggle"), exist_ok=True)
    shutil.copy(kaggle_json_path, os.path.expanduser("~/.kaggle/kaggle.json"))
    os.chmod(os.path.expanduser("~/.kaggle/kaggle.json"), 0o600)
except FileNotFoundError:
    print(f"Error: {kaggle_json_path} not found in Downloads.")
    exit(1)
except Exception as e:
    print(f"Error setting up Kaggle API: {e}")
    exit(1)

try:
    with zipfile.ZipFile(dataset_zip_path, "r") as zip_ref:
        zip_ref.extractall("atp_data")
except FileNotFoundError:
    print(f"Error: {dataset_zip_path} not found in Downloads.")
    exit(1)
except Exception as e:
    print(f"Error extracting dataset: {e}")
    exit(1)

try:
    files = glob.glob("atp_data/*.csv")
    if not files:
        print("Error: No CSV files found in 'atp_data' directory.")
        exit(1)
    dfs = [pd.read_csv(f) for f in files if f.endswith(".csv")]
    df = pd.concat(dfs, ignore_index=True)
    print("Initial DataFrame loaded. Rows:", len(df), "Columns:", df.columns.tolist())
    
    print("Unique tourney_date values:", df['tourney_date'].astype(str).unique()[:10])
    
    df_filtered = df[df['tourney_date'].astype(str).str.startswith(('2018','2019','2020','2021','2022','2023', '2024', '2025'))]
    if len(df_filtered) == 0:
        print("Warning: No matches found for 2023-2025. Using all data.")
        df_filtered = df
    else:
        print(f"Filtered to 2023-2025 matches: {len(df_filtered)} rows")
    df = df_filtered
except Exception as e:
    print(f"Error loading CSV files: {e}")
    exit(1)

selected_columns = [
    'w_1stWon', 'l_1stWon',
    'w_2ndWon', 'l_2ndWon',
    'w_ace', 'l_ace',
    'w_bpSaved', 'l_bpSaved',
    'winner_rank_points', 'loser_rank_points'
]

try:
    df_valid = df[selected_columns].dropna()
    print(f"Filtered DataFrame: {len(df_valid)} rows after excluding missing values.")
    if len(df_valid) == 0:
        print(f"Error: No valid rows after filtering for {selected_columns}. Check dataset columns and data quality.")
        print("Available columns:", df.columns.tolist())
        exit(1)
except KeyError as e:
    print(f"Error: One or more columns in {selected_columns} not found in dataset. Available columns: {df.columns.tolist()}")
    exit(1)

try:
    df_processed = df_valid.join(df[['winner_name', 'loser_name']], how='inner')
    print(f"Processed DataFrame: {len(df_processed)} rows after merging with winner_name and loser_name.")
    if len(df_processed) == 0:
        print("Error: No rows after merging winner_name and loser_name. Check data quality.")
        exit(1)
except KeyError as e:
    print(f"Error: 'winner_name' or 'loser_name' not found in dataset. Available columns: {df.columns.tolist()}")
    exit(1)

df_processed['winner_rank_points'] = np.log1p(df_processed['winner_rank_points']) * 1.2
df_processed['loser_rank_points']  = np.log1p(df_processed['loser_rank_points'])  * 1.2

cols_left  = ['w_1stWon','w_2ndWon','w_ace','w_bpSaved','winner_rank_points']
cols_right = ['l_1stWon','l_2ndWon','l_ace','l_bpSaved','loser_rank_points']

X_pos = df_processed[cols_left + cols_right].to_numpy()
y_pos = np.ones(len(X_pos), dtype=int)

X_neg = df_processed[cols_right + cols_left].to_numpy()
y_neg = np.zeros(len(X_neg), dtype=int)

X = np.vstack([X_pos, X_neg])
y_binary = np.concatenate([y_pos, y_neg])

print("Paired dataset constructed. Shapes:", X.shape, y_binary.shape)

np.savez('tennis_data.npz', X=X, y=y_binary)
print("Processed data saved to 'tennis_data.npz'")

data = np.load('tennis_data.npz')
X = data['X']
y_binary = data['y']

X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X, y_binary, test_size=0.15, random_state=42, stratify=y_binary
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train_raw)
X_test  = scaler.transform(X_test_raw)

def add_noise(X, noise_prob=0.01):
    mask = np.random.rand(*X.shape) < noise_prob
    X_noisy = X.copy()
    defaults = scaler.transform(np.array([
        [0, 0, 0, 0, np.log1p(500) * 1.2, 0, 0, 0, 0, np.log1p(500) * 1.2]
    ]))
    for i in range(X.shape[1]):
        X_noisy[mask[:, i], i] = defaults[0, i]
    return X_noisy

X_train_noisy = add_noise(X_train, noise_prob=0.01)

sample_weights = np.ones(len(X_train_raw))
rank_diff = np.abs(X_train_raw[:, 4] - X_train_raw[:, 9])
sample_weights = 1.0 + 0.5 * rank_diff

model = XGBClassifier(
    n_estimators=225,
    max_depth=4,
    learning_rate=0.03,
    random_state=42,
    eval_metric='logloss'
)

model.fit(X_train_noisy, y_train, sample_weight=sample_weights)

test_accuracy = model.score(X_test, y_test)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

feature_names = ['w_1stWon', 'w_2ndWon', 'w_ace', 'w_bpSaved', 'winner_rank_points', 'l_1stWon', 'l_2ndWon', 'l_ace', 'l_bpSaved', 'loser_rank_points']
importances = model.feature_importances_
print("Feature Importances:")
for name, imp in zip(feature_names, importances):
    print(f"{name}: {imp:.4f}")

def temperature_scaling(probabilities, temperature=1.15):
    return np.clip(probabilities / temperature, 0, 1)

def load_player_stats(player_name):
    normalized_name = normalize_name(player_name).lower()
    if normalized_name in player_stats_cache:
        stats = player_stats_cache[normalized_name]
        if (stats.get('first_serve_points_won') is not None and
            stats.get('second_serve_points_won') is not None and
            stats.get('aces_per_match') is not None and
            stats.get('breaks_per_match_bo3') is not None and
            stats.get('rank_points') is not None):
            return stats
        print(f"Invalid stats for {player_name} in cache. Skipping.")
        return None

    if not os.path.exists(stats_json_path):
        print(f"Error: {stats_json_path} not found.")
        return None

    try:
        with open(stats_json_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
    except Exception as e:
        print(f"Error reading {stats_json_path}: {e}")
        return None

    for player in all_data:
        if player['name'].lower() == normalized_name:
            if (player.get('first_serve_points_won') is not None and
                player.get('second_serve_points_won') is not None and
                player.get('aces_per_match') is not None and
                player.get('breaks_per_match_bo3') is not None and
                player.get('rank_points') is not None):
                player_stats_cache[normalized_name] = {
                    'name': player['name'],
                    'first_serve_points_won': player.get('first_serve_points_won'),
                    'second_serve_points_won': player.get('second_serve_points_won'),
                    'aces_per_match': player.get('aces_per_match'),
                    'breaks_per_match_bo3': player.get('breaks_per_match_bo3'),
                    'rank_points': float(player.get('rank_points'))
                }
                return player_stats_cache[normalized_name]
            print(f"Invalid stats for {player_name} in JSON. Skipping.")
            return None
    print(f"Player {player_name} not found in stats. Skipping.")
    return None

def predict_match(player1_name, player2_name, scale_factors=None):
    if player1_name.lower() == player2_name.lower():
        print(f"Error: {player1_name} and {player2_name} are the same player.")
        return None

    player1_stats = load_player_stats(player1_name)
    player2_stats = load_player_stats(player2_name)

    if not player1_stats or not player2_stats:
        print(f"Error: Could not retrieve valid stats for {player1_name} or {player2_name}. Skipping match.")
        return None

    scale_1st_serve = scale_factors.get('1st_serve_won', 0.90) if scale_factors else 0.90
    scale_2nd_serve = scale_factors.get('2nd_serve_won', 0.70) if scale_factors else 0.70
    scale_aces = scale_factors.get('aces', 0.90) if scale_factors else 0.90
    scale_breaks = scale_factors.get('breaks_per_match', 0.90) if scale_factors else 0.90
    scale_rank = scale_factors.get('rank_points', 0.98) if scale_factors else 0.98

    try:
        player1_data = [
            float(player1_stats['first_serve_points_won']) * scale_1st_serve,
            float(player1_stats['second_serve_points_won']) * scale_2nd_serve,
            float(player1_stats['aces_per_match']) * scale_aces,
            float(player1_stats['breaks_per_match_bo3']) * scale_breaks,
            np.log1p(float(player1_stats['rank_points'])) * scale_rank
        ]
        player2_data = [
            float(player2_stats['first_serve_points_won']) * scale_1st_serve,
            float(player2_stats['second_serve_points_won']) * scale_2nd_serve,
            float(player2_stats['aces_per_match']) * scale_aces,
            float(player2_stats['breaks_per_match_bo3']) * scale_breaks,
            np.log1p(float(player2_stats['rank_points'])) * scale_rank
        ]
        print(f"{player1_name} raw stats: {player1_stats}")
        print(f"{player1_name} scaled features: {player1_data}")
        print(f"{player2_name} raw stats: {player2_stats}")
        print(f"{player2_name} scaled features: {player2_data}")
    except ValueError as e:
        print(f"Error: Invalid stats format for {player1_name} or {player2_name}: {e}. Skipping match.")
        return None

    match_features_1 = np.array([player1_data + player2_data])
    match_features_2 = np.array([player2_data + player1_data])
    match_features_scaled_1 = scaler.transform(match_features_1)
    match_features_scaled_2 = scaler.transform(match_features_2)
    
    probability_1 = model.predict_proba(match_features_scaled_1)[:, 1][0]
    probability_2 = model.predict_proba(match_features_scaled_2)[:, 1][0]
    print(f"Raw probabilities - {player1_name}: {probability_1:.2%}, {player2_name}: {probability_2:.2%}")
    probability_1 = temperature_scaling(probability_1, temperature=1.15)
    probability_2 = 1 - temperature_scaling(probability_2, temperature=1.15)
    probability = (probability_1 + probability_2) / 2
    
    winner = player1_name if probability > 0.5 else player2_name
    print(f"Match: {player1_name} vs {player2_name}")
    print(f"Predicted winner: {winner} with probability {probability:.2%}\n")
    return {'match': f"{player1_name} vs {player2_name}", 'predicted_winner': winner, 'probability': f"{probability:.2%}"}

try:
    print("Enter matches (format: Player1+Player2, one per line).")
    print("Press Enter twice or Ctrl+D/Ctrl+Z to finish.")
    matches = []
    while True:
        try:
            line = input().strip()
            if not line:
                break
            matches.append(line)
        except EOFError:
            break

    if not matches:
        print("No matches entered. Exiting.")
        driver.quit()
        exit(0)

    players = set()
    for match in matches:
        try:
            player1, player2 = match.split('+')
            player1, player2 = player1.strip(), player2.strip()
            if player1 and player2:
                players.add(player1)
                players.add(player2)
        except:
            print(f"Error: Invalid format for match '{match}'. Skipping.")
            continue
    print(f"\nScraping stats for {len(players)} players: {', '.join(players)}\n")

    all_data, valid_players = scrape_and_save_stats(players)

    if not valid_players:
        print("No valid players found. Exiting.")
        driver.quit()
        exit(0)

    scale_factors = check_feature_alignment(df_processed, valid_players)

    print("\nModel training complete.")
    print(f"Processing {len(matches)} matches...\n")
    predictions_log = []
    for match in matches:
        try:
            players = match.split('+')
            if len(players) != 2:
                predictions_log.append({
                    'match': match,
                    'predicted_winner': 'Error',
                    'probability': 'Invalid format'
                })
                print(f"Error: Invalid format for match '{match}'. Use 'Player1+Player2'.\n")
                continue
            player1, player2 = [p.strip() for p in players]
            if not player1 or not player2:
                predictions_log.append({
                    'match': match,
                    'predicted_winner': 'Error',
                    'probability': 'Empty player name'
                })
                print(f"Error: Empty player name in match '{match}'.\n")
                continue
            if player1 not in valid_players or player2 not in valid_players:
                predictions_log.append({
                    'match': f"{player1} vs {player2}",
                    'predicted_winner': 'Error',
                    'probability': 'Missing or invalid stats'
                })
                print(f"Error: {player1} or {player2} has missing or invalid stats. Skipping match.\n")
                continue
            result = predict_match(player1, player2, scale_factors)
            if result:
                predictions_log.append(result)
            else:
                predictions_log.append({
                    'match': f"{player1} vs {player2}",
                    'predicted_winner': 'Error',
                    'probability': 'Invalid stats'
                })
        except Exception as e:
            predictions_log.append({
                'match': match,
                'predicted_winner': 'Error',
                'probability': str(e)
            })
            print(f"Error processing match '{match}': {e}\n")

    try:
        with open('predictions_log.csv', 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=['match', 'predicted_winner', 'probability'])
            writer.writeheader()
            writer.writerows(predictions_log)
        print("Predictions saved to predictions_log.csv")
    except Exception as e:
        print(f"Error saving predictions_log.csv: {e}")
except Exception as e:
    print(f"Error during execution: {e}")
finally:
    driver.quit()
    print("Execution complete.")

